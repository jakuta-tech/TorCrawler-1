#!/usr/bin/perl
# download the urls given in a file
# the job folder should be provided through commandline

use strict;
use warnings;
use Getopt::Long;
use Data::Dumper;
use Digest::MD5 qw(md5_hex);

my $workDir = ".";
my $pageDir = "page";
my $requstDelay = 2;
my $numLines;
my $numBatches;
my $splitSize;

# no proxy by default
my %proxyScore = ();

GetOptions("work-dir=s" => \$workDir) or die $!;

-d $workDir or die "working directory does not exist: $workDir";
chdir $workDir;

print ">>> creating sub folder for downloaded pages\n";
-d $pageDir or mkdir $pageDir;

# now process the url file and the url status

my $urlStatusFile = "url.status";

# urls to crawl which is generated from $urlStatusFile
my $urlInputFile = "url.in";
my $urlOutputFile = "url.out";
# url output file which is either generated by Fetcher or parser

# status priority
my @statusList = qw(DL_NEW DL_ERR1 DL_ERR2 DL_ERR3 DL_ERR4 DL_ERR5 DL_OK DL_EXTRACTED PS_OK PS_ERR);
my %statusOrderMap = map {$statusList[$_] => $_} 0 .. $#statusList;

# start downloading
fetch_url();

sub fetch_url{
	# get list of proxy to use
	read_proxy_list();
	while(1){
		update_url_status();
		extract_links();
		start_batch_download();
	}
}


sub start_batch_download{
	# first split the url input into parts
	my @goodProxies = grep {$proxyScore{$_} > 0.5} keys %proxyScore;
	$numBatches = scalar @goodProxies;
	$numLines = `wc -l url.in|cut -f1 -d' ' `;
	# remove trailing line changing
	chomp $numLines;
	if($numLines == 0){
		print ">>>>>>>>>>>>>>>>>>> [NOTICE]: no more url to download, exit now >>>>>>>>>>>>>>>>>>>\n";
		exit;
	}
	# when there are more proxies than jobs....
	if($numLines <= $numBatches){
		$numBatches = $numLines;
	}
	$splitSize = int($numLines / $numBatches);
	print ">>> # of urls to download: $numLines, batch size: $splitSize\n";
	print ">>> remove existing task split files\n";
	`find . -type f -name "url.in.[0-9][0-9]" |xargs -I {} rm {}`;

	`split -dl $splitSize url.in url.in.`;
	# now start the batch downloader
	my @pids = ();
	for(my $i = 0; $i < $numBatches; $i++){
		my $urlFile = sprintf("url.in.%02d",$i);
		my $proxy = $goodProxies[$i];
		my $logFile = sprintf("url.out.%02d.log",$i);
		my $outUrlFile = sprintf("url.out.%02d",$i);
		# construct download command line, wait: 2 seconds
		my $cmd = "TorGet.php -d$pageDir -w$requstDelay " . ($proxy ? "-p$proxy -h127.0.0.1" : "") . " < $urlFile > $outUrlFile 2>$logFile";
		print ">>> start to download batch $i: $cmd\n";
		my $pid = fork();
		if($pid){
			# append the pid to the file
			print ">>> downloading thread spawned with pid: $pid\n";
			push @pids, $pid;
			`echo $pid >> fetch.pids`;
		} else{
			# run the command and exit
			system($cmd);
			exit;
		}
	}
	print ">>> wait for downloading processes to finish\n";
	# wait until one of the child process returns and kill the rest
	while((my $returnPid = wait()) >= 0){
		# kill all pids
		print ">>> downloading thread with $returnPid returned, kill the rest\n";
		kill 'KILL', @pids;
	}
	print ">>> downloading done\n";
}

sub read_proxy_list{
	print ">>> read proxy score\n";
	open FILE, "<", "proxy.list" or die $!;
	while(<FILE>){
		chomp;
		my($port,$accuracy) = split /\t/;
		defined $accuracy or $accuracy = 1;
		$proxyScore{$port} = $accuracy;
	}
}

sub write_proxy_list{
	print ">>> write proxy score to file\n";
	open FILE, ">", "proxy.list" or die $!;
	my @sortedPorts = sort {$proxyScore{$b} <=> $proxyScore{$a} } keys %proxyScore;
	foreach(@sortedPorts){
		print FILE sprintf("%d\t%.2f\n",$_,$proxyScore{$_});
	}
	close FILE;
}


# extract links from downloaded html files
sub extract_links{
	# generate input url list, only consider urls of status DL_OK
	`grep -P "DL_OK" url.status |cut -f1 > lx.in`;
	# split into batches and run parser in parallel
	my $numUrls = `wc -l lx.in|cut -f1 -d' '`;
	chomp $numUrls;
	my $numTasks = 4;
	if($numUrls < $numTasks){
		$numTasks = $numUrls;
	}
	if($numUrls == 0){
		print STDERR "[NOTICE]: no urls to extract\n";
		return;
	}

	my $splitSize = int($numUrls / $numTasks);
	# archive log and remove files generated by previous round
	print ">>> cleaninup link extraction result\n";
	`rm lx.out`;
	`find . -type f -name "lx.out*.log" |xargs -I {} cat {}>> all.log`;
	`find . -type f -name "lx.out*"|xargs -I {} rm {}`;
	`find . -type f -name "lx.in.[0-9]*" |xargs -I {} rm {}`;
	`split -d -l $splitSize lx.in lx.in.`;
	my @pids = ();
	for(my $i = 0; $i < $numTasks; $i++){
		my $cmd = sprintf("RegexLX.pl --page-dir=$pageDir < lx.in.%02d 1> lx.out.%02d 2> lx.out.%02d.log", $i,$i,$i);
		print ">>> $cmd\n";
		my $pid = fork();
		if($pid){
			push @pids, $pid;
		} else{
			# run the command and quit
			`$cmd`;
			exit;
		}
	}
	# check pid status
	print ">>> wait for child processes to finish\n";
	my $startTime = time();
	while(wait() >= 0){
		# sleep(3);
	}
	my $time = time() - $startTime;
	print ">>> link extraction done, time consumed: $time seconds\n";

	# now update the url.status
	print ">>> concatenate all lx.out.xx files\n";
	`find . -type f -name "lx.out.[0-9][0-9]" |xargs -I {} cat {} >> lx.out`;
	# reset the url status map
	my %urlStatus = ();
	open FILE, "<", "url.status" or die $!;
	while(<FILE>){
		chomp;
		my($url,$status) = split /\t/;
		$urlStatus{$url} = $status;
	}	
	close FILE;

	# open the link extraction result
	open FILE, "<", "lx.out" or die $!;
	while(<FILE>){
		chomp;
		my($url,$status) = split /\t/;
		if(!exists $urlStatus{$url} || ($statusOrderMap{$urlStatus{$url}} < $statusOrderMap{$status})) {
			$urlStatus{$url} = $status;
		}
	}	
	close FILE;
	# dump the result
	open FILE, ">", "tmp.url.status" or die $!;
	my %toDownload = (
		"DL_NEW" => 1,
		"DL_ERR1" => 1,
		"DL_ERR2" => 1,
		"DL_ERR3" => 1
	);

	open IN_FILE, ">", "tmp.url.in" or die $!;
	while(my($url,$status) = each %urlStatus){
		print FILE join("\t",($url,$status)) . "\n";
		if(exists $toDownload{$status}){
			print IN_FILE join("\t",($url,$status)) . "\n";
		}
	}
	close FILE;
	close IN_FILE;
	`mv tmp.url.in url.in`;
	`mv tmp.url.status url.status`;
}

sub update_url_status{
	# update url status file by merging with the output
	# read in all url status
	print ">>> start to update url status\n";
	my %urlStatus = ();
	open FILE, "<", $urlStatusFile or die $!;
	while(<FILE>){
		chomp;
		my($url,$status) = split /\t/;
		(defined $status and $status and exists $statusOrderMap{$status}) or die "illegal status detected in the url.status file and it needs to be fixed immediately!";
		$urlStatus{$url} = $status;
	}
	# print Dumper(\%urlStatus);
	close FILE;

	# concatenate all url.out* files
	print ">>> concatenate all url output files by each crawling job\n";
	`find ./ -type f -name "url.out.[0-9][0-9]"|xargs -I {} cat {} >> tmp.url.out`;
	`find . -type f -name "url.out*.log" |xargs -I {} cat {} >> all.log`;
	# remove all url.out files
	print ">>> remove all url output files if any\n";
	`find ./ -type f -name "url.out.[0-9]*"|xargs -I {} rm {}`;
	# now read the tmp.url.out
	open FILE, "<", "tmp.url.out" or die $!;
	open STATUS_FILE, ">", "tmp.url.status" or die $!;
	open IN_FILE, ">", "tmp.url.in" or die $!;

	# track proxy accuracy
	my %tmpProxyScore = ();
	while(<FILE>){
		chomp;
		my($url,$status, $port) = split /\t/;
		# remove port: prefix
		$port =~ s/port\://;
		# only replace when current status is more recent
		if((!exists $urlStatus{$url}) || ($statusOrderMap{$urlStatus{$url}} < $statusOrderMap{$status})){
			$urlStatus{$url} = $status;
		}
		if((index $status, "DL_OK") >= 0){
			$tmpProxyScore{$port}->[0] ++;
		}elsif((index $status, "DL_ERR") >= 0){
			$tmpProxyScore{$port}->[1] ++;
		}
	}
	close FILE;
	print ">>> remove tmp url.out\n";
	`rm tmp.url.out`;

	print ">>> write the updated url status to tmp.url.status\n";
	my %toDownload = (
			"DL_NEW" => 1,
			"DL_ERR1" => 1,
			"DL_ERR2" => 1,
			"DL_ERR3" => 1
		);
	while(my($url,$status) = each %urlStatus){
		# write the result to 
		print STATUS_FILE join("\t",($url,$status)) . "\n";	
		exists $toDownload{$status} and print IN_FILE join("\t",($url,$status)) . "\n";
	}
	close STATUS_FILE;
	close IN_FILE;
	# rename it!
	print ">>> update url.status file\n";
	`mv tmp.url.status url.status`;
	`mv tmp.url.in url.in`;

	# update the proxy quality list
	print ">>> update proxy score\n";
	while(my($port,$cnt) = each %tmpProxyScore){
		my $total = 0;
		map {$total += $_} @$cnt;
		$proxyScore{$port} = $cnt->[0]/$total;
	}
	# write to file
	write_proxy_list();
}